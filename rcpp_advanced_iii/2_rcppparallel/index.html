<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>2. RcppParallel - SC2</title>
    <meta property="og:title" content="2. RcppParallel - SC2">
    
    <meta name="twitter:card" content="summary">

    
      
    

    
      
      <meta property="description" content="Here we briefly introduce the RcppParallel R package. As explained in the previous section, Rcpp and R’s C API are not guaranteed to be thread-safe, hence calling them within parallel code is ‘for &amp;hellip;">
      <meta property="og:description" content="Here we briefly introduce the RcppParallel R package. As explained in the previous section, Rcpp and R’s C API are not guaranteed to be thread-safe, hence calling them within parallel code is ‘for &amp;hellip;">
      
    

    
    

    

    
    


<link href='//cdn.bootcss.com/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



    <link rel="stylesheet" href="/sc2-2019/css/style.css" />
    <link rel="stylesheet" href="/sc2-2019/css/fonts.css" />
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Arvo">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Marcellus">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Code+Pro">

<link rel="stylesheet" href="/sc2-2019/css/custom.css" />

<link rel="icon" href="/sc1-2019/favicon.ico" type="image/x-icon" />























<nav class="breadcrumbs">
    
        <a href="https://mfasiolo.github.io/sc2-2019/">home / </a>
    
        <a href="https://mfasiolo.github.io/sc2-2019/rcpp_advanced_iii/">rcpp_advanced_iii / </a>
    
        <a href="https://mfasiolo.github.io/sc2-2019/rcpp_advanced_iii/2_rcppparallel/">2_rcppparallel / </a>
    
</nav>

  </head>

  
  <body class="sc2-2019">
    <header class="masthead">
      <h1><a href="/sc2-2019/">SC2</a></h1>

<p class="tagline">Statistical Computing 2</p>

      <nav class="menu">
  <input id="menu-check" type="checkbox" />
  <label id="menu-label" for="menu-check" class="unselectable">
    <span class="icon close-icon">✕</span>
    <span class="icon open-icon">☰</span>
    <span class="text">Menu</span>
  </label>
  <ul>
  
  
  <li><a href="/sc2-2019/">Home</a></li>
  
  <li><a href="/sc2-2019/rcpp/">Integrating R and C</a></li>
  
  <li><a href="/sc2-2019/rcpp_advanced_i/">Advanced Rcpp I</a></li>
  
  <li><a href="/sc2-2019/rcpp_advanced_ii/">Advanced Rcpp II</a></li>
  
  <li><a href="/sc2-2019/rcpp_advanced_iii/">Parallel Rcpp</a></li>
  
  
  </ul>
</nav>

    </header>

    <article class="main">
      <header class="title">
      
<h1>2. RcppParallel</h1>

<h3>
</h3>
<hr>


      </header>





<div id="TOC">
<ul>
<li><a href="#thread-safe-accessors">Thread-safe accessors</a></li>
<li><a href="#parallel-for-loops-with-rcppparallel">Parallel for loops with RcppParallel</a></li>
<li><a href="#parallel-reductions-with-rcppparallel">Parallel reductions with RcppParallel</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<style>
body {
text-align: justify}
</style>
<p>Here we briefly introduce the <code>RcppParallel</code> R package. As explained in the previous section, Rcpp and R’s C API are not guaranteed to be thread-safe, hence calling them within parallel code is ‘for experts only’. <code>RcppParallel</code> provides tools to access R vectors and matrices in a thread-safe way, thus making parallel coding easier. It also provides simple tools to parallelise your code at a higher level of abstraction, e.g. without explicitly handling parallel threads. Here we introduce the library via some basic examples, for more details see <a href="https://rcppcore.github.io/RcppParallel/">RcppParallel’s website</a>.</p>
<div id="thread-safe-accessors" class="section level3">
<h3>Thread-safe accessors</h3>
<p>Consider the problem of computing the <a href="https://en.wikipedia.org/wiki/Error_function">error function</a>. In <code>R</code> this can be done by:</p>
<pre class="r"><code>x &lt;- rnorm(1e5)
2 * pnorm(x * sqrt(2)) - 1</code></pre>
<p>An Rcpp function for doing this is:</p>
<pre class="r"><code>library(Rcpp)
sourceCpp(code = &#39;
#include &lt;boost/math/special_functions/erf.hpp&gt;
#include &lt;Rcpp.h&gt;
using namespace Rcpp;

// [[Rcpp::depends(BH)]]

// [[Rcpp::export(erf)]]
NumericVector erf(NumericVector x)
{

 size_t n = x.size();
 NumericVector out(n);
 
 for(size_t ii = 0; ii &lt; n; ii++)
 {
  out[ii] = boost::math::erf(x[ii]);
 }
 
 return out;

 }
&#39;)</code></pre>
<p>Note that we use the error function defined in the <code>Boost</code> C++ library, which can be accessed via the <code>BH</code> package. Let’s see whether it works:</p>
<pre class="r"><code>x &lt;- rnorm(1e6)
max(abs( (2 * pnorm(x * sqrt(2)) - 1) - erf(x)))</code></pre>
<pre><code>## [1] 3.330669e-16</code></pre>
<p>The numerical difference seems tolerable, however:</p>
<pre class="r"><code>library(microbenchmark)
options(microbenchmark.unit=&quot;relative&quot;)
microbenchmark(R = 2 * pnorm(x * sqrt(2)) - 1, 
               erf(x), 
               times = 100)</code></pre>
<pre><code>## Unit: relative
##    expr      min       lq     mean   median       uq      max neval
##       R 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000   100
##  erf(x) 1.504754 1.470644 1.430474 1.435553 1.425279 1.222098   100</code></pre>
<p>our <code>Rcpp</code> function does not seem very efficient. Let’s see whether we can do any better by parallelising the code via <code>RcppParalel</code> and OpenMP. In particular, consider the function:</p>
<pre class="r"><code>library(Rcpp)
sourceCpp(code = &#39;
#include &lt;boost/math/special_functions/erf.hpp&gt;
#include &lt;Rcpp.h&gt;
#include &lt;RcppParallel.h&gt;
using namespace Rcpp;

// [[Rcpp::depends(BH)]]
// [[Rcpp::depends(RcppParallel)]]
// [[Rcpp::plugins(openmp)]]

// [[Rcpp::export(erfOmp)]]
NumericVector erfOmp(NumericVector x, int ncores)
{

 size_t n = x.size();
 NumericVector out(n);
 RcppParallel::RVector&lt;double&gt; wo(out);
 RcppParallel::RVector&lt;double&gt; wx(x);
 
 #if defined(_OPENMP)
  #pragma omp parallel for num_threads(ncores)
 #endif
 for(size_t ii = 0; ii &lt; n; ii++)
 {
  wo[ii] = boost::math::erf(wx[ii]);
 }
 
 return out;

 }
&#39;)</code></pre>
<p>Note that the main loop has been parallelised via the <code>OpenMP</code> directive:</p>
<pre class="cpp"><code>#pragma omp parallel for num_threads(ncores)</code></pre>
<p>which we have already seen in a previous section. However, within the parallel for loop, we access the input (<code>x</code>) and output (<code>out</code>) vectors via two wrappers of class <code>RVector&lt;double&gt;</code>. The <code>RVector</code> class provides wrappers around Rcpp vectors, which can be accessed in a thread-safe way. Importantly, no copy is taken. Let us test the function:</p>
<pre class="r"><code>x &lt;- rnorm(1e6)
max(abs( (2 * pnorm(x * sqrt(2)) - 1) - erfOmp(x, 4)))</code></pre>
<pre><code>## [1] 4.440892e-16</code></pre>
<p>Looks close enough. On my Intel i7-3820 3.60GHz CPU with 4 cores 8 threads I get the following relative performance:</p>
<pre class="r"><code>microbenchmark(R = 2 * pnorm(x * sqrt(2)) - 1, 
               erfOmp(x, 1), 
               erfOmp(x, 4),
               erfOmp(x, 8),
               times = 100)</code></pre>
<pre class="r"><code>Unit: relative
         expr      min       lq     mean   median       uq       max neval
            R 3.951898 3.717663 3.388519 3.683268 3.173645 1.8356637   100
 erfOmp(x, 1) 5.690604 5.098202 4.574459 5.023652 4.261512 1.5492959   100
 erfOmp(x, 4) 1.462215 1.462516 1.471631 1.668728 1.501872 0.5660757   100
 erfOmp(x, 8) 1.000000 1.000000 1.000000 1.000000 1.000000 1.0000000   100</code></pre>
<p>So our Rcpp code is not very efficient, but using 8 threads reduces the computational time by a factor of over 3, relative to the basic R code.</p>
<p>Here we simply used the <code>RVector</code> class from <code>RcppParallel</code>, but the latter offers also a <code>RMatrix</code> class, which is a thread-safe accessor for Rcpp matrices (e.g., <code>NumericMatrix</code>). See <a href="https://gallery.rcpp.org/articles/parallel-matrix-transform/">here</a> for an example.</p>
</div>
<div id="parallel-for-loops-with-rcppparallel" class="section level3">
<h3>Parallel for loops with RcppParallel</h3>
<p>So far we simply used the <code>RVector</code> wrapper provided by <code>RcppParallel</code>, now we aim at exploiting also its parallelisation tools. Before doing that, consider the following function:</p>
<pre class="r"><code>sourceCpp(code = &#39;
#include &lt;boost/math/special_functions/erf.hpp&gt;
#include &lt;Rcpp.h&gt;

// [[Rcpp::depends(BH)]]

double myFun(double y){
  return boost::math::erf(y);
} 

// [[Rcpp::export(erfStd)]]
Rcpp::NumericVector erfStd(Rcpp::NumericVector x) {
  
  Rcpp::NumericVector out(x.length());
  
  std::transform(x.begin(), x.end(), out.begin(), myFun);
  
  return out;
}
&#39;)</code></pre>
<p>which is analogous to our original <code>erf</code> function, but now we use <code>std::trasform</code> in place of the explicit for loop. The performance is not much different:</p>
<pre class="r"><code>microbenchmark(R = 2 * pnorm(x * sqrt(2)) - 1, 
               erfStd(x),
               times = 100)</code></pre>
<pre class="r"><code>Unit: relative
      expr      min       lq     mean  median     uq      max neval
         R 1.000000 1.000000 1.000000 1.00000 1.0000 1.000000  1000
 erfStd(x) 1.441561 1.441671 1.413499 1.44087 1.4398 1.244145  1000</code></pre>
<p>but now its easier to explain the next step, which entails using <code>RcppParallel</code> to parallelise the computation. In particular, consider the following function:</p>
<pre class="r"><code>sourceCpp(code = &#39;
#include &lt;boost/math/special_functions/erf.hpp&gt;
#include &lt;Rcpp.h&gt;
#include &lt;RcppParallel.h&gt;
using namespace RcppParallel;

// [[Rcpp::depends(BH)]]
// [[Rcpp::depends(RcppParallel)]]

double myFun(double y){
  return boost::math::erf(y);
} 

struct ErfVec : public Worker
{
   const RVector&lt;double&gt; in;
   
   RVector&lt;double&gt; out;
   
   ErfVec(const Rcpp::NumericVector in_, Rcpp::NumericVector out_) 
      : in(in_), out(out_) {}
   
   void operator()(std::size_t begin, std::size_t end) {
      std::transform(in.begin() + begin, 
                     in.begin() + end, 
                     out.begin() + begin, 
                     myFun);
   }
};

// [[Rcpp::export(erfPar)]]
Rcpp::NumericVector erfPar(Rcpp::NumericVector x) {
  
  Rcpp::NumericVector out(x.length());
  
  ErfVec obj(x, out);
  
  parallelFor(0, x.length(), obj);
  
  return out;
}
&#39;)</code></pre>
<p>Before explaining how this works, let’s check whether it gives correct results:</p>
<pre class="r"><code>max(abs( (2 * pnorm(x * sqrt(2)) - 1) - erfPar(x)))</code></pre>
<pre><code>## [1] 4.440892e-16</code></pre>
<p>and let’s check its computational performance:</p>
<pre class="r"><code>microbenchmark(R = 2 * pnorm(x * sqrt(2)) - 1, 
               erfOmp(x, 4),
               erfPar(x),
               times = 100)</code></pre>
<pre class="r"><code>Unit: relative
         expr      min       lq     mean   median       uq      max neval
            R 3.865710 4.034473 3.667210 3.697507 3.548906 1.734676   100
 erfOmp(x, 4) 1.428727 1.425952 1.413645 1.384761 1.364873 1.092303   100
    erfPar(x) 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000   100</code></pre>
<p>which seems quite good! Now, let’s examine the key steps of <code>erfPar</code>. The parallel for loop is performed by the function:</p>
<pre class="cpp"><code>void parallelFor(std::size_t begin,
                 std::size_t end, 
                 Worker&amp; worker,
                 std::size_t grainSize = 1)</code></pre>
<p>defined in the <code>RcppParallel</code> namespace. The first two arguments define the beginning and end of the <code>for</code> loop, the third is an object of type <code>Worker</code> (which we will discuss below) and the last argument determines the minimal chunk size for parallelization. That is, if we set <code>grainSize = 10</code>, then each thread will perform at least 10 iterations of the for loop. In the code above, an argument of type <code>ErfVec</code> is passed to <code>RcppParallel</code>, via the <code>Worker</code> argument. Objects of type <code>ErfVec</code> are data structures which inherit the <code>RcppParallel::Worker</code> type. The structure has two elements or members:</p>
<pre class="cpp"><code>const RVector&lt;double&gt; in;
RVector&lt;double&gt; out;</code></pre>
<p>which will be used to store the input and output vectors. The members are initialised by the constructor:</p>
<pre class="cpp"><code>ErfVec(const Rcpp::NumericVector in_, Rcpp::NumericVector out_) 
      : in(in_), out(out_) {}</code></pre>
<p>which is called when we do:</p>
<pre class="cpp"><code>Rcpp::NumericVector out(x.length());
ErfVec obj(x, out);</code></pre>
<p>Hence, we allocate memory for the output vector and then we pass it to the <code>ErfVec</code> constructor, which will wrap it within an <code>RVector</code>. Then, the structure contains the operator:</p>
<pre class="cpp"><code>void operator()(std::size_t begin, std::size_t end) {
    std::transform(in.begin() + begin, 
                   in.begin() + end, 
                   out.begin() + begin, 
                   myFun);
}</code></pre>
<p>which will be called by <code>parallelFor</code> on a section of the input vector delimited by <code>begin</code> and <code>end</code>.</p>
<p>If we look at the source code in <code>RcppParallel</code>, we see that <code>parallelFor</code> is itself a wrapper around a call to:</p>
<pre class="cpp"><code>tbb::parallel_for(tbb::blocked_range&lt;size_t&gt;(begin, end, grainSize), 
                     tbbWorker);</code></pre>
<p>which is defined in Intel’s <a href="https://github.com/oneapi-src/oneTBB">Threading Building Blocks (TBB)</a> C++ library and shipped with <code>RcppParallel</code>. TBB is available on Windows, OS X, Linux, and Solaris x86, while on other platforms <code>parallelFor</code> will fall back on:</p>
<pre class="cpp"><code>void ttParallelFor(std::size_t begin, std::size_t end, 
                          Worker&amp; worker, std::size_t grainSize = 1)</code></pre>
<p>which is a less performing version, based on the (more portable) <a href="https://tinythreadpp.bitsnbites.eu/">TinyThread++</a> library (also shipped via <code>RcppParallel</code>).</p>
</div>
<div id="parallel-reductions-with-rcppparallel" class="section level3">
<h3>Parallel reductions with RcppParallel</h3>
<p>Consider the problem of calculating the log-likelihood of a sample <span class="math inline">\(x_1, \dots, x_n\)</span> under a Gaussian model. In <code>R</code> we would do it by:</p>
<pre class="r"><code>x &lt;- rnorm(10)
sum(dnorm(x, 0.5, 2, log = TRUE))</code></pre>
<p>The following code provides an <code>Rcpp</code> implementation:</p>
<pre class="r"><code>sourceCpp(code = &#39; 
#include &lt;Rcpp.h&gt;
#include &lt;cmath&gt;
using namespace Rcpp;

// [[Rcpp::plugins(cpp14)]] 

// [[Rcpp::export]]
double dnormSeq(const NumericVector x, const double m, const double s) {

  auto gld = [m_ = m, s_ = s] (const double x1, const double x2) { 
   return x1 - (x2-m_) * (x2-m_) / (s_*s_*2.0); 
  };

  double out = std::accumulate(x.begin(), x.end(), 0.0, gld);
  
  out -= x.length() * ::log(2 * M_PI * s * s) / 2;
  
  return out;
}
&#39;)</code></pre>
<p>Here these lines:</p>
<pre class="cpp"><code> auto gld = [m_ = m, s_ = s] (const double x1, const double x2) { 
   return x1 - (x2-m_) * (x2-m_) / (s_*s_*2.0); 
  };</code></pre>
<p>define a parametrised function, <code>gld</code>, with parameters <code>m_</code> and <code>s_</code>, which evaluates the log-density of a normal at <span class="math inline">\(x_2\)</span> (as in <code>dnorm(x2, m_, s_)</code>) and adds it to <code>x1</code>. In particular, here we are using a C++ <a href="https://stackoverflow.com/questions/7627098/what-is-a-lambda-expression-in-c11">lambda function</a> to fix parameters <code>m_</code> and <code>s_</code> and to create a function with explicit arguments <code>x1</code> and <code>x2</code>, which can then be passed to <code>std::accumulate</code>. The equivalent R code is:</p>
<pre class="r"><code># Create closure where m_ and s_ are fixed
funCreator &lt;- function(m_, s_){
  function(x1, x2)  x1 - (x2-m_) * (x2-m_) / (s_*s_*2.0)
}
gld &lt;- funCreator(0.5, 1) # Analogous to lambda function in C++

# Perform reduction
x &lt;- rnorm(10)
Reduce(gld, x) # Analogous to std::accumulate</code></pre>
<pre><code>## [1] -3.51904</code></pre>
<p>Some of the features we are using to define the lambda function are defined in the C++14 standard, hence we added the <code>Rcpp::plugins(cpp14)</code> attribute. Let’s see whether the <code>Rcpp</code> function works:</p>
<pre class="r"><code>x &lt;- rnorm(1e6)
dnormSeq(x, 0.5, 2)</code></pre>
<pre><code>## [1] -1768361</code></pre>
<pre class="r"><code>sum(dnorm(x, 0.5, 2, log = TRUE))</code></pre>
<pre><code>## [1] -1768361</code></pre>
<p>it seems so, and it is much quicker than base <code>R</code>:</p>
<pre class="r"><code>microbenchmark(R = sum(dnorm(x, 0.5, 2, log = TRUE)), 
               dnormSeq(x, 0.5, 2))</code></pre>
<pre class="r"><code>Unit: relative
                expr      min       lq     mean   median       uq      max neval
                   R 12.60742 12.38649 12.55583 12.25376 12.39503 11.30485   100
 dnormSeq(x, 0.5, 2)  1.00000  1.00000  1.00000  1.00000  1.00000  1.00000   100</code></pre>
<p>Now, consider the following parallel version:</p>
<pre class="r"><code>sourceCpp(code = &#39; 
#include &lt;boost/math/special_functions/erf.hpp&gt;
#include &lt;Rcpp.h&gt;
#include &lt;RcppParallel.h&gt;
using namespace RcppParallel;

// [[Rcpp::depends(BH)]]
// [[Rcpp::depends(RcppParallel)]]

// [[Rcpp::plugins(cpp14)]] 
  
struct Gau : public Worker
{
   const RVector&lt;double&gt; input;
   
   const double m;
   const double s;
  
   double value;
   
   Gau(const Rcpp::NumericVector input, const double m_, const double s_) : input(input), m(m_), s(s_), value(0) { }
   Gau(const Gau&amp; obj, Split) : input(obj.input), m(obj.m), s(obj.s), value(0) { }

   void operator()(std::size_t begin, std::size_t end) {
      auto gld = [m1 = m, s1 = s](const double x1, const double x2) { 
       return x1 - (x2-m1) * (x2-m1) / (s1*s1*2.0); 
      };
      value += std::accumulate(input.begin() + begin, input.begin() + end, 0.0, gld);
   }

   void join(const Gau&amp; rhs) {
      value += rhs.value;
   }
};

// [[Rcpp::export]]
double dnormPar(Rcpp::NumericVector x, double m, double s) {

   Gau obj(x, m, s);

   parallelReduce(0, x.length(), obj);

   return obj.value - x.length() * ::log(2 * M_PI * s * s) / 2;
}

&#39;)</code></pre>
<p>Before explaining how this works, let’s see whether it produces correct results:</p>
<pre class="r"><code>x &lt;- rnorm(1e6)
dnormPar(x, 0.5, 2)</code></pre>
<pre><code>## [1] -1768463</code></pre>
<pre class="r"><code>sum(dnorm(x, 0.5, 2, log = TRUE))</code></pre>
<pre><code>## [1] -1768463</code></pre>
<p>which seems to be the case. Let’s look at the speed-up, which on the CPU described above is:</p>
<pre class="r"><code>microbenchmark(R = sum(dnorm(x, 0.5, 2, log = TRUE)), 
               RcppSeq = dnormSeq(x, 0.5, 2), 
               RcppPar = dnormPar(x, 0.5, 2))</code></pre>
<pre class="r"><code>Unit: relative
    expr       min        lq      mean    median        uq       max neval
       R 48.132613 47.623084 47.100746 47.014235 46.438556 43.741062   100
 RcppSeq  3.820691  3.851518  3.677526  3.635321  3.570393  3.351935   100
 RcppPar  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   100</code></pre>
<p>So on a 4 cores machine we are over 3 times faster. Let’s try to understand how this works. The <code>dnormPar</code> function is relatively similar to the <code>erfPar</code> example, described above. In particular, <code>std::accumulate</code> has been substituted by</p>
<pre class="cpp"><code>parallelReduce(0, x.length(), obj);</code></pre>
<p>which is defined in the <code>RcppParallel</code> namespace and has the same arguments as <code>parallelFor</code>. However, here the structure of the worker <code>obj</code> is a bit more complex, let us examine it. Objects of type <code>Gaus</code> have members:</p>
<pre class="cpp"><code>const RVector&lt;double&gt; input;
const double m;
const double s;
double value;</code></pre>
<p>The first is the input vector, the second and third are the mean and standard deviation of the Gaussian, and the last is the value of the log-likelihood accumulated so far. The object has two constructors, the first is:</p>
<pre class="cpp"><code>Gau(const Rcpp::NumericVector input, const double m_, const double s_) : 
  input(input), m(m_), s(s_), value(0) { }</code></pre>
<p>This is the constructor being called when we do <code>Gau obj(x, m, s);</code> in <code>dnormPar</code>. The second constructor is:</p>
<pre class="cpp"><code>Gau(const Gau&amp; obj, Split) : input(obj.input), m(obj.m), s(obj.s), value(0) { }</code></pre>
<p>and will be called by <code>parallelReduce</code>. Note that it takes a reference to an object of type <code>Gau</code> as input, and it uses its members (e.g. <code>obj.input</code>) to initialise its own members. The <code>Split</code> argument is used by <code>parallelReduce</code> to split the computation. Then, we have the operator:</p>
<pre class="cpp"><code>void operator()(std::size_t begin, std::size_t end) {
 auto gld = [m1 = m, s1 = s](const double x1, const double x2) { 
  return x1 - (x2-m1) * (x2-m1) / (s1*s1*2.0); 
 };
 value += std::accumulate(input.begin() + begin, input.begin() + end, 0.0, gld);
}</code></pre>
<p>which is used to perform the evaluation and accumulation of the log-likelihood (as done in the sequential version, above), but only on a subset of the input vector. Note that we create the lambda function <code>gld</code> at this point, with argument <code>m1</code> and <code>s1</code> fixed to their values (<code>m</code> and <code>s</code>) within the <code>Gau</code> object. The join operator:</p>
<pre class="cpp"><code>void join(const Gau&amp; rhs) {
    value += rhs.value;
}</code></pre>
<p>is used by <code>parallelReduce</code> to perform the reduction.</p>
<p>It might be possible to accelerate the <code>dnormPar</code> further. In fact, if we substitute</p>
<pre class="cpp"><code>auto gld = [m1 = m, s1 = s]</code></pre>
<p>with</p>
<pre class="cpp"><code>auto gld = [m1 = 0.5, s1 = 2.0]</code></pre>
<p>and recompile, we get a much better performance:</p>
<pre class="r"><code>Unit: relative
    expr       min       lq       mean    median         uq       max neval
       R 167.53804 152.9173 123.351021 131.14373 114.322668 29.540746   100
 RcppSeq  13.31754  12.3467   9.513437  10.03514   8.662867  1.953358   100
 RcppPar   1.00000   1.0000   1.000000   1.00000   1.000000  1.000000   100</code></pre>
<p>Understanding why this is the case might be a good exercise (I don’t know the answer).</p>
<p>Here we illustrated some basic tools provided by <code>RcppParallel</code>, see the <a href="https://rcppcore.github.io/RcppParallel/index.html">package webpage</a> for more details.</p>
</div>
<div id="references" class="section level3">
<h3>References</h3>
<ul>
<li><p>Allaire, J.J., François, R., Ushey, K., Vandenbrouck, G. and Geelnard, M., Intel (2018)
RcppParallel: Parallel Programming Tools for Rcpp. R package version 4.4.2.</p></li>
<li><p>Chandra, R., Dagum, L., Kohr, D., Menon, R., Maydan, D. and McDonald, J., 2001.
Parallel programming in OpenMP. Morgan kaufmann.</p></li>
<li><p>Chapman, B., Jost, G. and Van Der Pas, R., 2008.
Using OpenMP: portable shared memory parallel programming (Vol. 10). MIT press.</p></li>
<li><p>Eddelbuettel, D., 2019. Parallel Computing With R: A Brief Review. arXiv preprint arXiv:1912.11144.</p></li>
</ul>
</div>


  <footer>
  

<script src="//yihui.name/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>


<script type="text/javascript">
var sc_project=12110974;
var sc_invisible=1;
var sc_security="9b171880";
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img
class="statcounter"
src="https://c.statcounter.com/12110974/0/9b171880/1/"
alt="Web Analytics"></a></div></noscript>








  


<p align=right>

<a href='https://github.com/mfasiolo/sc2-2019/blob/master/content/rcpp_advanced_III/2_RcppParallel.Rmd'>View source</a>

|

<a href='https://github.com/mfasiolo/sc2-2019/edit/master/content/rcpp_advanced_III/2_RcppParallel.Rmd'>Edit source</a>

</p>





<script src="https://utteranc.es/client.js"
        repo="awllee/sc1-2019"
        issue-term="pathname"
        label="utterance"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>



  



<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/tex.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



  
  <hr>
  <div class="copyright">© 2020 <a href="https://mfasiolo.github.io/">Matteo Fasiolo</a></div>
  
  </footer>
  </article>
  
  </body>
</html>

