<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>3. Example on multivariate KDE with RcppArmadillo - SC2</title>
    <meta property="og:title" content="3. Example on multivariate KDE with RcppArmadillo - SC2">
    
    <meta name="twitter:card" content="summary">

    
      
    

    
      
      <meta property="description" content="To provide a more challeging example illustrating the capabilities of RcppArmadillo, here we show how to perform multivariate kernel density estimation (k.d.e.) using this library. In particular, let &amp;hellip;">
      <meta property="og:description" content="To provide a more challeging example illustrating the capabilities of RcppArmadillo, here we show how to perform multivariate kernel density estimation (k.d.e.) using this library. In particular, let &amp;hellip;">
      
    

    
    

    

    
    


<link href='//cdn.bootcss.com/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



    <link rel="stylesheet" href="/sc2-2019/css/style.css" />
    <link rel="stylesheet" href="/sc2-2019/css/fonts.css" />
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Arvo">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Marcellus">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Code+Pro">

<link rel="stylesheet" href="/sc2-2019/css/custom.css" />

<link rel="icon" href="/sc1-2019/favicon.ico" type="image/x-icon" />























<nav class="breadcrumbs">
    
        <a href="https://mfasiolo.github.io/sc2-2019/">home / </a>
    
        <a href="https://mfasiolo.github.io/sc2-2019/rcpp_advanced_i/">rcpp_advanced_i / </a>
    
        <a href="https://mfasiolo.github.io/sc2-2019/rcpp_advanced_i/3_kde_armadillo/">3_kde_armadillo / </a>
    
</nav>

  </head>

  
  <body class="sc2-2019">
    <header class="masthead">
      <h1><a href="/sc2-2019/">SC2</a></h1>

<p class="tagline">Statistical Computing 2</p>

      <nav class="menu">
  <input id="menu-check" type="checkbox" />
  <label id="menu-label" for="menu-check" class="unselectable">
    <span class="icon close-icon">✕</span>
    <span class="icon open-icon">☰</span>
    <span class="text">Menu</span>
  </label>
  <ul>
  
  
  <li><a href="/sc2-2019/">Home</a></li>
  
  <li><a href="/sc2-2019/rcpp/">Integrating R and C</a></li>
  
  <li><a href="/sc2-2019/rcpp_advanced_i/">Advanced Rcpp I</a></li>
  
  <li><a href="/sc2-2019/rcpp_advanced_ii/">Advanced Rcpp II</a></li>
  
  
  </ul>
</nav>

    </header>

    <article class="main">
      <header class="title">
      
<h1>3. Example on multivariate KDE with RcppArmadillo</h1>

<h3>
</h3>
<hr>


      </header>





<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#an-r-based-and-a-dumb-rcpp-solution">An R-based and a dumb <code>Rcpp</code> solution</a></li>
<li><a href="#a-better-rcpparmadillo-solution">A better <code>RcppArmadillo</code> solution</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<style>
body {
text-align: justify}
</style>
<div id="introduction" class="section level3">
<h3>Introduction</h3>
<p>To provide a more challeging example illustrating the capabilities of <code>RcppArmadillo</code>, here we show how to perform multivariate kernel density estimation (k.d.e.) using this library. In particular, let <span class="math inline">\({\bf x}_1^o, \dots, {\bf x}_n^o\)</span> be <span class="math inline">\(d\)</span>-dimensional vectors, sampled from the density <span class="math inline">\(\pi(\bf x)\)</span>. A k.d.e. estimate of <span class="math inline">\(\pi({\bf x})\)</span> is
<span class="math display">\[
\hat{\pi}_{\bf H}({\bf x}) = \frac{1}{n} \sum_{i=1}^n \kappa_{\bf H}({\bf x} - {\bf x}_i^o),
\]</span>
where <span class="math inline">\(\kappa_{\bf H}\)</span> is a kernel with positive definite bandwidth matrix <span class="math inline">\({\bf H}\)</span>. Here we assume that <span class="math inline">\(\kappa_{\bf H}\)</span> is a multivariate Gaussian kernel, with covariance matrix <span class="math inline">\(\kappa_{\bf H}\)</span>. Further, the data is generated from the mixture of warped Gaussians of <a href="https://arxiv.org/pdf/1611.06874.pdf">Fasiolo et al. (2018)</a>. The details of this density are unimportant here, what matters is that the example is defined in <span class="math inline">\(d&gt;2\)</span> dimensions and, under such density <span class="math inline">\(\pi({\bf x})\)</span>, the joint distribution of the first two dimensions is far from Gaussian while the remaining dimensions are standard i.i.d. Gaussian variables.</p>
<p>Samples from this density can be generated using the following functions:</p>
<pre class="r"><code>rBanana &lt;- function(n, d, a, b, shi1, shi2){
  out &lt;- matrix(rnorm(d*n), n, d);
  out[ , 1] &lt;- out[ , 1] * a
  out[ , 2] &lt;- out[ , 2] - b * (out[ , 1]^2 - a^2)
  out[ , 1] &lt;- out[ , 1] + shi1
  out[ , 2] &lt;- out[ , 2] + shi2
  return( out )
}

rBanMix &lt;- function(n, d, w, a, b, shi1, shi2){
  nmix &lt;- length( a );
  m &lt;- floor( n * w );
  m[1] &lt;- m[1] + n - sum(m);
  m &lt;- round(m);
  out &lt;- lapply(1:nmix, function(.ii)
                         rBanana(m[.ii], d, a[.ii], b[.ii], shi1[.ii], shi2[.ii])
               )
  out &lt;- do.call(&quot;rbind&quot;, out)
  return( out );
}</code></pre>
<p>which we use here to generate <span class="math inline">\(10^3\)</span> variables in <span class="math inline">\(5\)</span> dimensions:</p>
<pre class="r"><code>d &lt;- 5;
bananicity = c(0.2, -0.03, 0.1, 0.1, 0.1, 0.1)
sigmaBan &lt;- c(1, 6, 4, 4, 1, 1)
banShiftX &lt;- c(0, 0, 7, -7, 7, -7)
banShiftY &lt;- c(0, -5, 7, 7, 7.5, 7.5)
nmix &lt;- length(bananicity)
bananaW = c(1, 4, 2.5, 2.5, 0.5, 0.5)
bananaW &lt;- bananaW / sum(bananaW)

x &lt;- rBanMix(1e3, d, bananaW, sigmaBan, bananicity, banShiftX, banShiftY)

pairs(x)</code></pre>
<p><img src="/sc2-2019/rcpp_advanced_I/3_KDE_Armadillo_files/figure-html/unnamed-chunk-2-1.png" width="672" />
The plot demonstrates that only the first two dimensions of <span class="math inline">\(\bf x\)</span> are far from Gaussian. The next section presents a first attempt at estimating the density using <code>R</code> and <code>Rcpp</code>.</p>
</div>
<div id="an-r-based-and-a-dumb-rcpp-solution" class="section level3">
<h3>An R-based and a dumb <code>Rcpp</code> solution</h3>
<p>We start by creating a generic <code>R</code> function for kernel density estimation in <code>R</code>:</p>
<pre class="r"><code>kdeR &lt;- function(dker, y, x, H){
  n &lt;- nrow(x)
  m &lt;- nrow(y)
  out &lt;- numeric(m)
  for(ii in 1:n){
    out &lt;- out + dker(y, x[ii, ], H)
  }
  out &lt;- out/n
  return(out)
}</code></pre>
<p>This function works as follows:</p>
<ul>
<li><code>y</code> is the <span class="math inline">\(m \times d\)</span> matrix at points at which we want to evaluate the k.d.e.;</li>
<li><code>x</code> is the <span class="math inline">\(n \times d\)</span> matrix of original samples;</li>
<li><code>H</code> is the bandwith matrix.</li>
</ul>
<p>We are now ready to evaluate our k.d.e. a grid of <span class="math inline">\(10^4\)</span> points along the first two dimensions:</p>
<pre class="r"><code>l &lt;- 100
x1 &lt;- seq(-20, 20, length.out = l);
x2 &lt;- seq(-15, 15, length.out = l);
grd &lt;- as.matrix(cbind(expand.grid(x1, x2), matrix(0, l^2, d-2)))

library(mvtnorm)
dns &lt;- kdeR(dmvnorm, grd, x, diag(d))</code></pre>
<p>Where we use the <code>dmvnorm</code> function from the <code>mvtnorm</code> package to evaluate the Gaussian kernel and we set <span class="math inline">\(\bf H\)</span> simply to the identity matrix. We plot the <code>k.d.e.</code>:</p>
<pre class="r"><code>library(ggplot2)
library(viridis)</code></pre>
<pre><code>## Loading required package: viridisLite</code></pre>
<pre class="r"><code>ggplot(data = data.frame(x = grd[ , 1], y = grd[ , 2], z = dns),
       mapping = aes(x = x, y = y, z = z, fill = z)) +
       geom_raster() + geom_contour() + scale_fill_viridis_c()</code></pre>
<p><img src="/sc2-2019/rcpp_advanced_I/3_KDE_Armadillo_files/figure-html/unnamed-chunk-5-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>Can we speed the evaluation of the k.d.e. up using <code>RcppArmadillo</code>? We start with a “lazy” version, where we replace <code>kdeR</code> with:</p>
<pre class="r"><code>library(Rcpp)
sourceCpp(code = &#39;
// [[Rcpp::depends(RcppArmadillo)]]
#include &lt;RcppArmadillo.h&gt;
using namespace arma;

// [[Rcpp::export(name = &quot;kdeLazy&quot;)]]
Rcpp::NumericVector kde_i(Rcpp::Function dker, mat&amp; y, mat&amp; x, mat&amp; H) {
  unsigned int n = x.n_rows;
  unsigned int m = y.n_rows;
  vec out(m, fill::zeros);
  for(int ii = 0; ii &lt; n; ii++){
   out += Rcpp::as&lt;vec&gt;(dker(y, x.row(ii), H));
  }
  out /= n;
  return Rcpp::wrap(out);
}&#39;)</code></pre>
<p>Which can be used identically to <code>kdeR</code> and produces the same estimates:</p>
<pre class="r"><code>plot(kdeR(dmvnorm, x, x, diag(d)),
     kdeLazy(dmvnorm, x, x, diag(d)))
abline(0, 1, col = 2)</code></pre>
<p><img src="/sc2-2019/rcpp_advanced_I/3_KDE_Armadillo_files/figure-html/unnamed-chunk-7-1.png" width="384" style="display: block; margin: auto;" />
Let us compare the performance of <code>kdeR</code> and <code>kdeLazy</code>:</p>
<pre class="r"><code>library(microbenchmark)
microbenchmark(kdeR = kdeR(dmvnorm, x, x, diag(d)),
               kdeLazy = kdeLazy(dmvnorm, x, x, diag(d)))</code></pre>
<pre><code>## Unit: milliseconds
##     expr      min       lq     mean   median       uq      max neval
##     kdeR 179.2743 185.4951 199.0932 200.8142 206.1937 274.7425   100
##  kdeLazy 210.4834 219.3579 232.9470 233.2664 241.8953 308.7470   100</code></pre>
<p>It turns out that the <code>Rcpp</code> version is not faster. This demonstrates that writing part of the code in <code>Rcpp</code> does not automatically lead to performance gains. In particular our <code>Rcpp</code> code:</p>
<ul>
<li>calls the <code>dmvnR</code> function to compute the Gaussian kernel. We cannot expect an <code>R</code> function
to be any faster when called from <code>C++</code> rather than from <code>R</code>!</li>
<li>the call to <code>Rcpp::as&lt;vec&gt;</code> in the main <code>for</code> loop makes a copy at each call, which is
inefficient.</li>
</ul>
<p>The next section proposes a “proper” <code>RcppArmadillo</code> solution for multivariate kernel density estimation.</p>
</div>
<div id="a-better-rcpparmadillo-solution" class="section level3">
<h3>A better <code>RcppArmadillo</code> solution</h3>
<p>We start by developing an efficient <code>RcppArmadillo</code> function for evaluating the multivariate normal density. Before thinking about its implementation in software, we have to think about what is the most efficient way of computing it numerically. The formula for the log-density is
<span class="math display">\[
\log \phi({\bf x}, \mu, {\bf H}) = -\frac{1}{2}({\bf x} - \mu)^T{\bf H}^{-1}({\bf x} - \mu) - \frac{d}{2} \log (2\pi) - \frac{1}{2}\log\text{det}({\bf H}). 
\]</span>
Now, for the purpose of maximizing efficiency, we have to think about how to numerically compute the quadratic form and the determinant. Let’s start with the first.</p>
<p>Remember that, in our k.d.e. application, <span class="math inline">\({\bf x}_1, \dots, {\bf x}_m\)</span> are the points at which we want to evaluate the k.d.e. and <span class="math inline">\(\mu_1 = {\bf x}^o_1, \dots, \mu_n = {\bf x}^o_n\)</span> represent the centers of the kernel densities. Hence, for any fixed value of <span class="math inline">\(\mu\)</span>, we will have to evaluate the quadratic form <span class="math inline">\(m\)</span> times. For fixed <span class="math inline">\(\mu\)</span>, we could consider two approaches:</p>
<ol style="list-style-type: upper-alpha">
<li><p>calculate the inverse of <span class="math inline">\({\bf H}\)</span> upfront and then calculate
<span class="math inline">\(({\bf x}_i - \mu)^T{\bf H}^{-1}({\bf x}_i - \mu)\)</span>, for <span class="math inline">\(i = 1, \dots, m\)</span>;</p></li>
<li><p>solve the linear system <span class="math inline">\({\bf H}^{-1}({\bf x}_i - \mu) = {\bf z}_i\)</span> and then do
<span class="math inline">\(({\bf x}_i - \mu)^T{\bf z}_i\)</span>, for <span class="math inline">\(i = 1, \dots, m\)</span>;</p></li>
</ol>
<p>Approach A seems quite appealing: we pay an upfront <span class="math inline">\(O(d^3)\)</span> cost to get <span class="math inline">\({\bf H}^{-1}\)</span> and the we pay <span class="math inline">\(O(d^2)\)</span> cost <span class="math inline">\(m\)</span> times to evaluate the quadratic forms. So, for <span class="math inline">\(d &lt;&lt; m\)</span>, the dominant cost is <span class="math inline">\(O(md^2)\)</span>. Approach B seems more wasteful: solving the linear system by Gaussian elimination will cost <span class="math inline">\(O(d^3)\)</span> operations, so doing it <span class="math inline">\(m\)</span> times brings the dominant cost to <span class="math inline">\(O(md^3)\)</span>. However, here it is essential to take into account the fact that <span class="math inline">\(\bf H\)</span> is positive definite, which means that approach B can be implemented much more efficiently. In particular, let <span class="math inline">\({\bf L}{\bf L}^T = {\bf H}\)</span> be the Cholesky decomposition of <span class="math inline">\(\bf H\)</span>. Then <span class="math inline">\({\bf H}^{-1}({\bf x}_i - \mu) = ({\bf L}^{-1})^T{\bf L}^{-1}({\bf x}_i - \mu)\)</span> can be computed by:</p>
<ol style="list-style-type: decimal">
<li>solving <span class="math inline">\({\bf L}^{-1}({\bf x}_i - \mu) = {\bf z}_i\)</span> by forward-substitution
(recall that <span class="math inline">\(\bf L\)</span> is lower triangular). This has cost <span class="math inline">\(O(d^2)\)</span>.</li>
<li>solving <span class="math inline">\(({\bf L}^{-1})^T {\bf z}_i = \tilde{\bf z}_i\)</span> by back-substitution. Again <span class="math inline">\(O(d^2)\)</span>.</li>
<li>computing <span class="math inline">\(({\bf x}_i - \mu)^T \tilde{\bf z}_i\)</span> which is <span class="math inline">\(O(d)\)</span>.</li>
</ol>
<p>Hence, the cost of approach B is <span class="math inline">\(O(m d^2)\)</span> as well, but has the advantage of avoiding matrix inversion, which is generally less accurate than solving a linear system.</p>
<p>The approach we are going to follow is based on the observation that <span class="math inline">\(({\bf x}_i - \mu)^T {\bf H}^{-1}({\bf x}_i - \mu) = {\bf z}_i^T{\bf z}_i\)</span>, hence once we have obtained <span class="math inline">\({\bf z}_i\)</span> by forward-substitution as in step 1 above, we just need to calculate its squared norm at cost <span class="math inline">\(O(d)\)</span>. Having obtained the cholesky decomposition of <span class="math inline">\(\bf H\)</span>, computing its log-determinant is very cheap because <span class="math inline">\(\log\text{det}({\bf H}) = \log\text{det}({\bf L}{\bf L}^T) = 2\log\text{det}({\bf L}) = 2\log\text{trace}({\bf L})\)</span>.
Before looking at the <code>Armadillo</code> function to evaluate the Gaussian log-density, which we call <code>dmvnInt</code>, lets look at the function that will be accessed at <code>R</code> level:</p>
<pre class="cpp"><code>Rcpp::NumericVector kde_i(mat&amp; y, mat&amp; x, mat&amp; H) {
  unsigned int n = x.n_rows;
  unsigned int m = y.n_rows;
  vec out(m, fill::zeros);
  mat cholDec = chol(H, &quot;lower&quot;);
  for(int ii = 0; ii &lt; n; ii++){
   out += dmvnInt(y, x.row(ii), cholDec);
  }
  out /= n;
  return Rcpp::wrap(out);
}</code></pre>
<p>This is almost identical to the function that we have used before, the main differences being the following lines:</p>
<ul>
<li><code>mat cholDec = chol(H, &quot;lower&quot;);</code> here we are computing the Cholesky decomposition of <span class="math inline">\(\bf H\)</span>
and we are storing the lower-triangular factor in <code>cholDec</code>;</li>
<li><code>out += dmvnInt(y, x.row(ii), cholDec);</code> evaluates a multivariate normal density, with mean vector
<span class="math inline">\(\mu = {\bf x}_i^0\)</span> and covariance <span class="math inline">\(\bf H\)</span>, at <span class="math inline">\({\bf x}_1, \dots, {\bf x}_m\)</span>. Instead of passing
<span class="math inline">\(\bf H\)</span> to <code>dmvnInt</code>, we pass it directly its lower-triangular factor. We assume that the output
<code>dmvnInt</code> is an <code>arma::vec</code>, which can be accumulated in <code>out</code> without calling <code>Rcpp::as&lt;vec&gt;</code>.</li>
</ul>
<p>Then, we examine the function for evaluating the multivariate Gaussian density:</p>
<pre class="cpp"><code>vec dmvnInt(mat &amp; X, const rowvec &amp; mu, mat &amp; L)
{
  unsigned int d = X.n_cols;
  unsigned int m = X.n_rows;
  
  vec D = L.diag();
  vec out(m);
  vec z(d);
  
  double acc;
  unsigned int icol, irow, ii;
  for(icol = 0; icol &lt; m; icol++)
  {
    for(irow = 0; irow &lt; d; irow++)
    {
     acc = 0.0;
     for(ii = 0; ii &lt; irow; ii++) acc += z.at(ii) * L.at(irow, ii);
     z.at(irow) = ( X.at(icol, irow) - mu.at(irow) - acc ) / D.at(irow);
    }
    out.at(icol) = sum(square(z));
  }

  out = exp( - 0.5 * out - ( (d / 2.0) * log(2.0 * M_PI) + sum(log(D)) ) );

  return out;
}</code></pre>
<p>the important lines are:</p>
<ul>
<li><code>vec dmvnInt(mat &amp; X, const rowvec &amp; mu, mat &amp; L)</code> we return an <code>arma::vec</code> and all
inputs are passed by reference;</li>
<li><code>vec D = L.diag(); vec out(m); vec z(d);</code> we extract the main diagonal of <code>L</code>, we defined the
<span class="math inline">\(m\)</span>-dimensional vector <code>out</code> will contain the density values at <span class="math inline">\({\bf x}_1, \dots, {\bf x}_m\)</span>
and the “working” <span class="math inline">\(d\)</span>-dimensional vector <code>z</code> which we will use in the main loop;</li>
<li><code>for(icol = 0; icol &lt; m; icol++)</code> here we loop across <span class="math inline">\({\bf x}_1, \dots, {\bf x}_m\)</span>.
We calculate <span class="math inline">\({\bf z}_i\)</span> using the nested loop (next bullet point) and then
we use <code>out.at(icol) = sum(square(z));</code> to compute and store <span class="math inline">\(||{\bf z}_i||^2\)</span>.</li>
<li><code>for(irow = 0; irow &lt; d; irow++)</code> given <span class="math inline">\({\bf x}_i\)</span> we get
<span class="math inline">\({\bf z}_i = {\bf L}^{-1}({\bf x}_i - \mu)\)</span> by forward-substitution.</li>
</ul>
<p>Let us now compile and load using <code>sourceCpp</code>:</p>
<pre class="r"><code>sourceCpp(code = &#39;
// [[Rcpp::depends(RcppArmadillo)]]
#include &lt;RcppArmadillo.h&gt;
using namespace arma;

vec dmvnInt(mat &amp; X, const rowvec &amp; mu, mat &amp; L)
{
  unsigned int d = X.n_cols;
  unsigned int m = X.n_rows;
  
  vec D = L.diag();
  vec out(m);
  vec z(d);
  
  double acc;
  unsigned int icol, irow, ii;
  for(icol = 0; icol &lt; m; icol++)
  {
    for(irow = 0; irow &lt; d; irow++)
    {
     acc = 0.0;
     for(ii = 0; ii &lt; irow; ii++) acc += z.at(ii) * L.at(irow, ii);
     z.at(irow) = ( X.at(icol, irow) - mu.at(irow) - acc ) / D.at(irow);
    }
    out.at(icol) = sum(square(z));
  }

  out = exp( - 0.5 * out - ( (d / 2.0) * log(2.0 * M_PI) + sum(log(D)) ) );

  return out;
}

// [[Rcpp::export(name = &quot;kdeArma&quot;)]]
Rcpp::NumericVector kde_i(mat&amp; y, mat&amp; x, mat&amp; H) {
  unsigned int n = x.n_rows;
  unsigned int m = y.n_rows;
  vec out(m, fill::zeros);
  mat cholDec = chol(H, &quot;lower&quot;);
  for(int ii = 0; ii &lt; n; ii++){
   out += dmvnInt(y, x.row(ii), cholDec);
  }
  out /= n;
  return Rcpp::wrap(out);
}&#39;)</code></pre>
<p>Notice that we are not exporting <code>dmvnInt</code>, which is used only internally by <code>kde_i</code>. Let’s test whether our <code>Armadillo</code> version works:</p>
<pre class="r"><code>plot(kdeR(dmvnorm, x, x, diag(d)),
     kdeArma(x, x, diag(d)))
abline(0, 1, col = 2)</code></pre>
<p><img src="/sc2-2019/rcpp_advanced_I/3_KDE_Armadillo_files/figure-html/unnamed-chunk-12-1.png" width="384" style="display: block; margin: auto;" />
It seems so! We now compare the <code>Armadillo</code> and the <code>R</code> version in terms of speed:</p>
<pre class="r"><code>library(microbenchmark)
microbenchmark(R = kdeR(dmvnorm, x, x, diag(d)),
               Armadillo = kdeArma(x, x, diag(d)))</code></pre>
<pre><code>## Unit: milliseconds
##       expr       min        lq      mean    median       uq       max neval
##          R 180.41633 186.28067 196.73678 193.56767 204.6908 257.86668   100
##  Armadillo  59.68322  60.02129  60.74227  60.50509  61.4103  62.78317   100</code></pre>
<p>This is definitely better than the previous version! Notice that the <code>R</code> version (<code>kdeR</code>) is not too inefficient here because, at each iteration of the main <code>for</code> loop, the <code>dmvnorm</code> function evaluates a Gaussian density at <span class="math inline">\(10^3\)</span> data points. If we reduce the number of points at which the k.d.e. is evaluated, the performance gains increase. For instance:</p>
<pre class="r"><code>library(microbenchmark)
microbenchmark(R = kdeR(dmvnorm, x[1:10, ], x, diag(d)),
               Armadillo = kdeArma(x[1:10, ], x, diag(d)))</code></pre>
<pre><code>## Unit: microseconds
##       expr        min          lq        mean      median        uq        max
##          R 106539.532 109767.9300 117944.5045 119489.2070 121811.39 175408.794
##  Armadillo    533.281    551.1755    578.0306    568.2265    594.66    689.821
##  neval
##    100
##    100</code></pre>
<p>This is attributable to a) the fact that we are computing the Cholesky decomposition of <span class="math inline">\(\bf H\)</span> only once rather than <span class="math inline">\(n\)</span> times, b) to the fact that <code>for</code> loops are much quicker in <code>C++</code> that in <code>R</code> and c) (hopefully) that our code for computing the multivariate normal density is faster than that used by <code>dmvtnorm</code>.</p>
</div>
<div id="references" class="section level3">
<h3>References</h3>
<ul>
<li><p>Armadillo project: <a href="http://arma.sourceforge.net/" class="uri">http://arma.sourceforge.net/</a></p></li>
<li><p>Conrad Sanderson and Ryan Curtin. Armadillo: a template-based C++ library for linear algebra.
Journal of Open Source Software, Vol. 1, pp. 26, 2016.</p></li>
<li><p>Dirk Eddelbuettel and Conrad Sanderson, “RcppArmadillo: Accelerating R with high-performance
C++ linear algebra”, Computational Statistics and Data Analysis, 2014, 71, March, pages 1054-
1063, <a href="http://dx.doi.org/10.1016/j.csda.2013.02.005" class="uri">http://dx.doi.org/10.1016/j.csda.2013.02.005</a>. )</p></li>
<li><p>Fasiolo, M., de Melo, F.E. and Maskell, S., 2018. Langevin incremental mixture importance sampling. Statistics and Computing, 28(3), pp.549-561.</p></li>
</ul>
</div>


  <footer>
  

<script src="//yihui.name/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>


<script type="text/javascript">
var sc_project=12110974;
var sc_invisible=1;
var sc_security="9b171880";
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img
class="statcounter"
src="https://c.statcounter.com/12110974/0/9b171880/1/"
alt="Web Analytics"></a></div></noscript>








  


<p align=right>

<a href='https://github.com/mfasiolo/sc2-2019/blob/master/content/rcpp_advanced_I/3_KDE_Armadillo.Rmd'>View source</a>

|

<a href='https://github.com/mfasiolo/sc2-2019/edit/master/content/rcpp_advanced_I/3_KDE_Armadillo.Rmd'>Edit source</a>

</p>





<script src="https://utteranc.es/client.js"
        repo="awllee/sc1-2019"
        issue-term="pathname"
        label="utterance"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>



  



<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/tex.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



  
  <hr>
  <div class="copyright">© 2019 <a href="https://mfasiolo.github.io/">Matteo Fasiolo</a></div>
  
  </footer>
  </article>
  
  </body>
</html>

